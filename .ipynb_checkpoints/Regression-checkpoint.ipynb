{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "##importing the required libariries##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import operator\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_244</th>\n",
       "      <th>f_245</th>\n",
       "      <th>f_246</th>\n",
       "      <th>f_247</th>\n",
       "      <th>f_248</th>\n",
       "      <th>f_249</th>\n",
       "      <th>f_250</th>\n",
       "      <th>f_251</th>\n",
       "      <th>f_252</th>\n",
       "      <th>f_253</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.066056</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>-1.833</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.115</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.607</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-1.244</td>\n",
       "      <td>-0.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.910473</td>\n",
       "      <td>1.179</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-1.243</td>\n",
       "      <td>1.985</td>\n",
       "      <td>...</td>\n",
       "      <td>1.282</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>1.281</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.821</td>\n",
       "      <td>-0.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.830711</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.778</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.887</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>1.872</td>\n",
       "      <td>-1.709</td>\n",
       "      <td>0.135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>1.073</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.570</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>1.435</td>\n",
       "      <td>1.332</td>\n",
       "      <td>-1.147</td>\n",
       "      <td>2.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.180862</td>\n",
       "      <td>0.745</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>1.163</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>0.225</td>\n",
       "      <td>1.223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>1.682</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.613</td>\n",
       "      <td>1.033</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.462784</td>\n",
       "      <td>1.217</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-2.873</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.654</td>\n",
       "      <td>1.230</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target    f_0    f_1    f_2    f_3    f_4    f_5    f_6    f_7    f_8  \\\n",
       "0  3.066056 -0.653  0.255 -0.615 -1.833 -0.736    NaN  1.115 -0.171 -0.351   \n",
       "1 -1.910473  1.179 -0.093 -0.556  0.811 -0.468 -0.005 -0.116 -1.243  1.985   \n",
       "2  7.830711  0.181 -0.778 -0.919  0.113  0.887 -0.762  1.872 -1.709  0.135   \n",
       "3 -2.180862  0.745 -0.245 -1.343  1.163 -0.169 -0.151 -1.100  0.225  1.223   \n",
       "4  5.462784  1.217 -1.324 -0.958  0.448 -2.873 -0.856  0.603  0.763  0.020   \n",
       "\n",
       "   ...    f_244  f_245  f_246  f_247  f_248  f_249  f_250  f_251  f_252  f_253  \n",
       "0  ...   -1.607 -1.400 -0.920 -0.198 -0.945 -0.573  0.170 -0.418 -1.244 -0.503  \n",
       "1  ...    1.282  0.032 -0.061    NaN -0.061 -0.302  1.281 -0.850  0.821 -0.260  \n",
       "2  ...   -0.237 -0.660  1.073 -0.193  0.570 -0.267  1.435  1.332 -1.147  2.580  \n",
       "3  ...    0.709 -0.203 -0.136 -0.571  1.682  0.243 -0.381  0.613  1.033  0.400  \n",
       "4  ...    0.892 -0.433 -0.877  0.289  0.654  1.230  0.457 -0.754 -0.025 -0.931  \n",
       "\n",
       "[5 rows x 255 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##importing the data into a pandas data frame##\n",
    "train_DF=pd.read_csv('codetest_train.txt', delimiter='\\t')\n",
    "train_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5000.000000\n",
       "mean        1.143878\n",
       "std         5.259896\n",
       "min       -26.705570\n",
       "25%        -2.034383\n",
       "50%         1.166835\n",
       "75%         4.439549\n",
       "max        26.347818\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aa563d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##getting the statistics of the target values to get a better feeling of the proper learning method##\n",
    "n, bins, patches = plt.hist(train_DF['target'], 50, normed=1, facecolor='green', alpha=0.75)\n",
    "train_DF['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##splitting the data into train/test set to have a set for performance evaluation##\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_DF.iloc[:,1:255], train_DF['target'].round(decimals=0), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##data_refine function will: 1-find the non-numeric columns, 2- fill the NaN in non-numeric columns with the most used element\n",
    "#3- transform the non-numeric columns to numeric values using LabelEncoder method from SKLearn, 4- fill the NaN values in the numeric\n",
    "#columns using the mean value of them\n",
    "def data_refine(train_DF):\n",
    "    #1-find the non-numeric columns#\n",
    "    real_or_str=train_DF.applymap(np.isreal).all(0)\n",
    "    non_num_feature=real_or_str[~real_or_str].keys()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for col in non_num_feature:\n",
    "        #2- fill the NaN in non-numeric columns with the most used element#\n",
    "        train_DF[col].fillna(train_DF[col].describe().top, inplace=True)\n",
    "        #3- transform the non-numeric columns to numeric values using LabelEncoder method from SKLearn#\n",
    "        train_DF[col]=le.fit_transform(train_DF[col])\n",
    "    #4- fill the NaN values in the numeric columns using the mean value of them\n",
    "    train_DF.fillna(train_DF.mean(), inplace=True)\n",
    "    return train_DF\n",
    "\n",
    "X_train_refined=data_refine(X_train)\n",
    "X_test_refined=data_refine(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the training set is:\n",
      "12.8529173766\n",
      "MSE on the test set is:\n",
      "18.4248539444\n"
     ]
    }
   ],
   "source": [
    "##Using a Support Vector Regressor as our learning technique##\n",
    "clf = svm.SVR(C=1, epsilon=0.2)\n",
    "clf.fit(X_train_refined, y_train) \n",
    "print (\"MSE on the training set is:\\n\"), mean_squared_error(y_train, clf.predict(X_train_refined))\n",
    "print (\"MSE on the test set is:\\n\"), mean_squared_error(y_test, clf.predict(X_test_refined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the training set is:\n",
      "2.60496\n",
      "MSE on the test set is:\n",
      "13.91709\n"
     ]
    }
   ],
   "source": [
    "##Using another learning technique (Random Forrest Regressor) on our data##\n",
    "rfr = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "rfr.fit(X_train_refined, y_train)\n",
    "print (\"MSE on the training set is:\\n\"), mean_squared_error(y_train, rfr.predict(X_train_refined))\n",
    "print (\"MSE on the test set is:\\n\"), mean_squared_error(y_test, rfr.predict(X_test_refined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, both of these models tend to overfit the training set. Linear Regressors/K Nearest Neighbor/Neural Networks yield the same performance and all tend to overfit as well. We will try two different method to overcome overfitting.**\n",
    "\n",
    "    1- We will try to use PCA to reduce the number of features\n",
    "    2- We will pick Random Forrest Regressor as our model and using the k-fold cross validation technique fine tune the parameters of the SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 50 features, 0.280985 of variance is preserved and the MSE on the test set is 37.761140\n",
      "With 100 features, 0.508647 of variance is preserved and the MSE on the test set is 36.271060\n",
      "With 150 features, 0.702114 of variance is preserved and the MSE on the test set is 36.428650\n",
      "With 200 features, 0.865518 of variance is preserved and the MSE on the test set is 35.243320\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "##Trying PCA and reducing the number of features to 100/150/200 and checking if the performance on the test set improves##\n",
    "for n in [50,100,150,200]:\n",
    "    pca = PCA(n_components=n)\n",
    "    rfr.fit(pca.fit_transform(X_train_refined),y_train)\n",
    "    print \"With %d features, %f of variance is preserved and the MSE on the test set is %f\"%(n, pca.explained_variance_ratio_.sum(), \\\n",
    "                                                mean_squared_error(y_test, rfr.predict(pca.fit_transform(X_test_refined)))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that almost all of the columns are containing significant amount of variance/data and even using 200 features we are still losing around 15% of the data. Also the performance of our model doesn't improve by using less features and it still overfit. So, next we try to fine tune the parameters of our RFR model using a 5-fold cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##trying different values for n_estimators parameters (number of trees in the forrest) and evaluating the performance \n",
    "#using a 5-folded cross validation on the training set##\n",
    "est_list=[10,20,50,100,200]\n",
    "score={}\n",
    "for est in est_list:\n",
    "    rfr = RandomForestRegressor(n_estimators=est, random_state=0)\n",
    "    rfr.fit(X_train_refined, y_train)\n",
    "    score[est]=cross_val_score(rfr, X_train_refined, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the optimal value of n_estimators=200\n"
     ]
    }
   ],
   "source": [
    "##finding the optimal value of n_estimators##\n",
    "n_est=max(score.iteritems(), key=operator.itemgetter(1))[0]\n",
    "print \"the optimal value of n_estimators=%d\"%(n_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on the training set using optimal value of n_estimators is:\n",
      "1.79734879375\n",
      "score on the test set using optimal value of n_estimators is:\n",
      "13.111648625\n"
     ]
    }
   ],
   "source": [
    "##Using a Random Forest Regressor with the optimal C and epsilon parameters##\n",
    "rfr = RandomForestRegressor(n_estimators=n_est, random_state=0)\n",
    "rfr.fit(X_train_refined, y_train)\n",
    "print (\"score on the training set using optimal value of n_estimators is:\\n\"), mean_squared_error(y_train, rfr.predict(X_train_refined))\n",
    "print (\"score on the test set using optimal value of n_estimators is:\\n\"), mean_squared_error(y_test, rfr.predict(X_test_refined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##predicting the target on the given test set and writing the output into a text file##\n",
    "final_test_DF=pd.read_csv('codetest_test.txt', delimiter='\\t')\n",
    "X_test_final_refined=data_refine(final_test_DF)\n",
    "predictions=rfr.predict(X_test_final_refined)\n",
    "f = open('predicted_target_test.txt', 'w')\n",
    "for pre in predictions:\n",
    "    f.writelines(str(pre)+'\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
